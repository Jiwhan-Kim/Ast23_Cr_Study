'''
* Asterisk Chromium Project 2024
* ACrP 2024
*
* AI Project since Asterisk Vanadium Project 2023
* Repository generated by Jiwhan-Kim, Yonsei Univeristy.
*
* A. Introduction to an Image Classifier.
'''

# Modules
import torch, torch.nn as nn, torch.optim as optim
from torch.utils.data import DataLoader, Dataset, random_split
from torchvision import transforms, datasets

import numpy as np
from tqdm import tqdm
from os import path

# Set Devices
device = None
if torch.cuda.is_available():
    device = "cudo:0"
elif torch.backends.mps.is_available():
    device = "mps"
else:
    device = "cpu"


"""
Tranformation Mode
1. Crop the image to 32x32 with padding 4 and allow reflections.
2. Randomly flip the image horizontally.
3. Transform the image to tensor.
4. Normalize the image with the given mean and standard deviation.
"""
transform = transforms.Compose(
    [
        transforms.RandomCrop(32, padding=4, padding_mode='reflect'),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))
    ]
)

def Loader_CIFAR10(n_train, batch_size):
    # Data Sets
    trainset = datasets.CIFAR10(root='../Datas', train=True, download=True, transform=transform)
    testset = datasets.CIFAR10(root='../Datas', train=False, download=True, transform=transform)
    n_eval = 50000 - n_train
    train_ds, valid_ds = random_split(trainset, [n_train, n_eval])

    train_load = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4)
    valid_load = DataLoader(valid_ds, batch_size=batch_size, shuffle=False, num_workers=4)
    test_load = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=4)

    print("Data Loading Completed")
    return train_load, valid_load, test_load

'''
Trainer
    Optimizer: Adam
    Loss Function: CrossEntropyLoss
    Scheduler: OneCycleLR
'''
class AC_Trainer:
    def __init__(self, model, device, max_lr, betas, weight_decay, epochs, train_load, label_smoothing=0, grad_clip=None):
        self.model = model
        self.device = device
        self.grad_clip = grad_clip
        self.lossF = nn.CrossEntropyLoss(label_smoothing=label_smoothing)
        self.optimizer = optim.Adam(self.model.parameters(),lr=max_lr, betas=betas, weight_decay=weight_decay)
        self.scheduler = optim.lr_scheduler.OneCycleLR(optimizer=self.optimizer, max_lr=max_lr, epochs=epochs, steps_per_epoch=len(train_load))

    def step(self, image: torch.tensor, label: torch.tensor) -> float:
        x  = image.to(self.device)
        y  = label.to(self.device)
        self.optimizer.zero_grad()
        output = self.model.forward(x)
        loss = self.lossF(output, y)
        loss.backward()

        if self.grad_clip:
            nn.utils.clip_grad_value_(self.model.parameters(), self.grad_clip)

        self.optimizer.step()
        self.scheduler.step()
        return loss

    def get_lr(self):
        for param_group in self.optimizer.param_groups:
            return param_group['lr']
        pass
'''
Model
    ResNet
'''
class BasicBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=stride, padding=1, bias=False) # stride = 1 or 2
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)

        self.shortcut = nn.Sequential()
        if stride != 1:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride, padding=0, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = nn.ReLU()(out)

        out = self.conv2(out)
        out = self.bn2(out)

        out = out + self.shortcut(residual)
        out = nn.ReLU()(out)

        return out
    
    def get_lr(self):
        for param_group in self.optimizer.param_groups:
            return param_group['lr']
        pass
    

class ResNet(nn.Module):
    def __init__(self):
        super(ResNet, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)

        self.layer1 = self.make_layer(64, 64, 1) # first block stride 1
        self.layer2 = self.make_layer(64, 128, 2)
        self.layer3 = self.make_layer(128, 256, 2)
        self.layer4 = self.make_layer(256, 512, 2)

        self.fc = nn.Linear(2*2*512, 10)


    def make_layer(self, in_channels, out_channels, stride):
        layers=[]
        if stride == 1:
            layers.append(BasicBlock(in_channels, out_channels, 1))
            layers.append(BasicBlock(out_channels, out_channels, 1))
        elif stride == 2:
            layers.append(BasicBlock(in_channels, out_channels, 2))
            layers.append(BasicBlock(out_channels, out_channels, 1))
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        
        x = self.bn1(x)
        x = nn.ReLU()(x)
        x = nn.MaxPool2d(2, 2)(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        x = x.view(x.shape[0], -1)
        x = self.fc(x)

        return x

'''
Main
'''
# Global Data
train_size = 40_000
batch_size = 128
epochs = 20

def train(loader, model, trainer, n_epoch):
    loss = 0
    model.train()
    pbar = tqdm(loader)
    for image, label in pbar:
        x = image.to(device)
        y = label.to(device)
        loss = trainer.step(x, y)
        pbar.set_description(f"Training Epoch {n_epoch:3d}, {loss:2.6f}")

def evaluate(loader, model, n_epoch):
    with torch.no_grad():
        correct = 0
        model.eval()
        result_pbar = tqdm(loader)
        for image, label in result_pbar:
            x = image.to(device)
            y = label.to(device)
            output = model.forward(x)
            result = torch.argmax(output, dim=1)
            for res, ans in zip(result, y):
                if res == ans:
                    correct += 1
        print("Epoch {}. Accuracy: {}".format(n_epoch, 100 * correct / (50000 - train_size)))

if __name__ == "__main__":
    print("A_Introduction.py")

    print("Device on Working: ", device)
    train_load, valid_load, test_load = Loader_CIFAR10(train_size, batch_size)
    
    model = ResNet().to(device)
    print("Your Model: ResNet")

    trainer = AC_Trainer(model, device, 0.001, (0.9, 0.999), 1e-6, epochs, train_load)
    model_params_file = "./model_params_ResNet.pth"
    
    if path.exists(model_params_file):
        model.load_state_dict(torch.load(model_params_file))
        print("Parameter Exists. Loading Completed")
    else:
        print("Parameter Doesnt Exists. Training Started")
        for i in range(epochs):
            train(train_load, model, trainer, i)
            evaluate(valid_load, model, i)
            print("Current Learning Rate", trainer.get_lr())
        torch.save(model.state_dict(), model_params_file)
        print("Parameter Saved")
    
    with torch.no_grad():
        model.eval()
        val = np.zeros(10, dtype=int)
        correct = 0

        result_pbar = tqdm(test_load)
        for image, label in result_pbar:
            x = image.to(device)
            y = label.to(device)
            output = model.forward(x)
            result = torch.argmax(output, dim=1)
            for res, ans in zip(result, y):
                if res == ans:
                    correct += 1
                    val[res] += 1

        print("Test Accuracy: ", 100 * correct / 10000)
        for i in range(10):
            print(f"Class {i}: {val[i]}")
    
    print("A_Introduction.py Completed")

